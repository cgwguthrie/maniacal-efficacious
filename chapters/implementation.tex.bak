\chapter{Schema Implementation}\label{implementation}
This chapter focuses on the implementation of the data models discussed in chapter \ref{design}. Each database management system has their own procedure for instantiating a new collection, table, node or column family. This chapter discusses the methods and strategies I imposed to create the database solution designs; with a focus on any challenges faced in doing so. Chapter \ref{dataload} evaluates the process undertaken to physically load the data into the systems.

\section{Creating database systems}\label{dbcreate}
Once the process of modelling of the database systems was complete, the next stage was to transform the model plan into actual databases. For each database system, this was relatively simple. However, with this simplicity, brought limitations and restrictions of which I had to construe, to fully achieve my target model.

\subsubsection*{MySQL}
The MySQL data model consists of 8 tables; AnatomyStructures, Assays, Genes, Publications, Sources, Specimens, Stages and TextAnnotations. Each of which are discussed in detail in section \ref{mysqldesign}. The creation of these tables was a relatively straightforward undertaking. The ease in which I found this process, may be due to my previous experience of using MySQL. Another rational explanation would be that the intuitive and logical way in which relational databases are constructed, make implementing a data model, an all round elementary procedure. An example of how a table is created in MySQL can be found in the code snippet \ref{code:mysqlass} below. This code illustrates the creation of the Assays table, the indexes and the constraints.
\newpage
\vspace*{\fill}
\begin{lstlisting}[language=SQL, caption=Creation of Assays table in MySQL., label=code:mysqlass]
--
-- Table structure for table `Assays`
--
CREATE TABLE IF NOT EXISTS `Assays` (
  `emage_id` int(11) NOT NULL,
  `type` varchar(255) DEFAULT NULL,
  `probe_id` varchar(255) DEFAULT NULL,
  `source_id` int(11) NOT NULL,
  `specimen_id` int(11) NOT NULL,
  `stage_id` int(11) NOT NULL,
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
--
-- Indexes for table `Assays`
--
ALTER TABLE `Assays`
  ADD PRIMARY KEY (`emage_id`),
  ADD KEY `source_id` (`source_id`),
  ADD KEY `specimen_id` (`specimen_id`),
  ADD KEY `stage_id` (`stage_id`);
--
-- Constraints for table `Assays`
--
ALTER TABLE `Assays`
  ADD CONSTRAINT `fk_Assays_Sources` FOREIGN KEY (`source_id`)
  REFERENCES `Sources` (`source_id`),
  ADD CONSTRAINT `fk_Assays_Specimens` FOREIGN KEY (`specimen_id`)
  REFERENCES `Specimens` (`id`),
  ADD CONSTRAINT `fk_Assays_Stages` FOREIGN KEY (`stage_id`)
  REFERENCES `Stages` (`id`);
\end{lstlisting}
\vspace*{\fill}
\newpage

As you can see in lines 4 - 11, the creation of each column takes the form of; column name, data type with length and the default attributes and values i.e. null or not null. The character set for each table, by default was set to utf8. You will notice here that the key values were not in fact instantiated at time of creation. This was as a result of an experiment to evaluate the affect the exclusion of index keys and constraints has at time of data load on each database. The findings of this experiment are illustrated in figure \ref{fig:index} with reference to the performance of the other database solutions for comparison.

Intuitively, one would expect quicker load times and a slower querying performance with no implemented indexes. Figure \ref{fig:index} clearly illustrates that the time taken to load the data without index keys and constraints had little, to no true affect on load performance. While there is slight variance in load time, this can be expected and attributed to a number of reasons, such as other processes running simultaneously on the CPU for example. For in-depth discussion and comparison of use cases, see section \ref{discussion}.

MySQL tables are linked by joining the (unique) primary key of one column to the (unique or non unique) foreign key of another. Lines 15-19 in code snippet \ref{code:mysqlass}, is where the key columns are created and lines 23 - 29 is where the foreign key constraints are expressed. The notion of keys joining tables can often be a slightly confusing concept to understand on first encounter. Primary and foreign keys are, not always, but in most cases confined to integer values. This is as a result of, data often containing inconsistent, ambiguous and non universal values. For example, a primary key may have the value ``Mouse'' and a foreign key may have the value ``mouse''. Both valid strings however as they do not match exactly the join would fail. The rigidity of these constructs have as many advantages as they do disadvantages. While the concept of joining two tables on matching integers seems logical, many situations occur where there is no unique ID present in the dataset and therefore the ID has to be manually created based on the data available.

The process to create multiple tables and join them together in MySQL is relatively straightforward. While the formality of definitively expressing each term and its data type then stipulating the index keys and constrains, can be a tedious process, it is done so in a logical and objective manner, which makes it coherent and understandable for the programmer.

\subsection*{MongoDB}
Creating a document inside a MongoDB collection (the equivalent to a MySQL table) is done by inserting field and value pairs. Each time a new field and value pair is inserted into a collection, a new document is created. By default, each document in a collection is provided with a unique ID which has an object data type. ObjectIds are small, fast to generate, and ordered. These values consists of 12-bytes, where the first four bytes are a timestamp that reflect the ObjectIdâ€™s creation \cite{mongo}. A unique ID can also be manually created for each document, if available. By this I mean, if a dataset has a unique identifier, which will be used as a reference, this can be implemented by definitively expressing the ``\_id'' field, to a value. For example ``\_id : 1234'' would by the ID of a single document. This concept is illustrated in line 2 of code snippet \ref{code:mongoinsert}.

MongoDB is an extremely flexible data store. It accepts multiple different data types, from the standard; string, double and boolean values to the more complex regular expressions and even Javascript code. By default, any value without a type cast will be presumed to be either a sting or integer value. This attribute is common of NoSQL systems. It adds to the simplicity and smooth process of implementing a data model.

Documents can be created by either manually inserting on the command line, or by conducting a data dump. The latter is discussed in more detail in section \ref{mongoload}. Code snippet \ref{code:mongoinsert} below, is an example of how a document can be created and data inserted from the command line.

Creating documents in MongoDB is an extremely simple process. One command and you can insert an entire database of information. While it is unlikely that one would manually insert a large volume of data using the \textit{db.collection.insert(\{\})} method, it is an available option. It is more likely that one would use this insertion process for adding additional data to an already implemented database, as opposed to creating from scratch. For example the EMAGE dataset contains around 200,000 entries. Inserting the full dataset using this methodology, while valid, would certainly not be the optimal solution. An explanation defining the full procedure into how I created the MongoDB documents can be found in section \ref{neoload}.

An important design decision which has to be taken prior to model implementation is, whether to create multiple collections or embed data within a document. A comprehensive evaluation of the advantages and disadvantages is discussed in section \ref{mongodesign}. The key difference between the two approaches is, that within a multiple collection database, querying is required to be undertaken at application level and is done by referencing data entities. Whereas in an embedded document, all of the data is available in a single schema and can be accessed by a single query. Thus improving query performance and application response time.

As discussed in section \ref{mongodesign} and illustrated in code snippet \ref{code:mongoinsert}, the MongoDB data model I have designed uses the embedded document approach. What do I mean when I say embedded? A good way of thinking of the embedded data concept is, a document in a document. For example, while values in MongoDB can be absolute, such as a string with value ``mouse'' they can also be an embedded document which in turn has values such as ``mouse''. Lines 6-34 of code snippet \ref{code:mongoinsert} represents an example of data, embedded in a document. Looking specifically at lines 14-20, we have a value namely ``publication'' which has embedded values of ``publicationID'', ``author'' and ``title''. Thus allowing direct querying of additional data, as opposed to messy collection joins.

\newpage
\begin{lstlisting}[language=json,caption=Example insertion of data into a MongoDB document., label=code:mongoinsert]
db.emage.insert({
    "_id" : 5354,
    "probeID" : "Flt1 probeA",
    "source" : "emage",
    "type" : "in situ",
    "specimen" : {
        "strain" : "unspecified",
        "type" : "wholemount"
    },
    "stage" : {
        "dpc" : "9.5 dpc",
        "theilerstage" : 15
    },
    "publication" : [ 
        {
            "publicationID" : 9113979,
            "author" : "Ema M, Taya S, Yokotani N, Sogawa K, Matsuda Y, Fujii-Kuriyama Y",
            "title" : "A novel bHLH-PAS factor with close sequence similarity to hypoxia-inducible factor 1alpha regulates the VEGF expression and is potentially involved in lung and vascular development."
        }
    ],
    "textannotation" : [ 
        {
            "anatomystructure" : {
                "term" : "cardiovascular system",
                "structureID" : 16104
            },
            "strength" : "detected",
            "gene" : {
                "name" : "Flt1",
                "geneID" : "MGI:95558"
            }
        }
    ]
})
\end{lstlisting} 


\subsection*{Neo4j}
To implement a Neo4j structure we use a query language namely Cypher Query Language (CQL). Cypher, is a declarative graph query language that allows for expressive and efficient querying and updating of a graph store \cite{nd}. CQL was designed to be as user friendly as possible for both programmers and operations professionals alike \cite{nd}. The structure of CQL is based upon SQL and shares many of its attributes. CQL queries are built using various clauses. For a detailed insight into CQL, see section \ref{neo}.

As Neo4j is based upon the property graph model, a database is implemented by constructing nodes and relationships. A node is made up of either a single or a number of properties. A property is a value which is named by a string. The accepted property values in Neo4j are: Numeric, String, Boolean and Collections of any other value type, for example, an array of Strings. By default, all values in Neo4j are of property type String. A type cast, has to be declared in order to manipulate the property value type. A relationship organises the nodes by joining two nodes together on a shared common value. As with nodes, relationships can have definitive value properties.

A node created in Neo4j is automatically instantiated with an ID value. Each and every node in the database has a unique numerical ID, which is incremented from the first node to the last inserted. This value can not be changed. Creating relationships between the nodes and querying the graph, is generally done by imposing various clauses upon the node values as opposed to this unique ID. However this ID value can indeed be used as or within a query clause by using the ``ID'' clause function.

Creating a Neo4j structure can either be done by implementing a Cypher file; containing the indexes, nodes, constraints and relationships of the data model. The Cypher file can then be bulk loaded into the database. Alternatively a data model can be manually implemented using the Neo4j shell command prompt. I will detail an example of how to create a structure from the command line. However, for the full EMAGE dataset instantiation, I created a Cypher file and imported the dataset simultaneously. The implementation of this is discussed in section \ref{neoload}, which also describes how to load data into Neo4j fom a CSV file. Code snippet \ref{code:neocreate} illustrates the implementation of a simple data structure in Neo4j.

\newpage
\vspace*{\fill}
\begin{lstlisting}[language=SQL, caption=Creation of example data structure in Neo4j., label=code:neocreate]

CREATE (matrix1:Movie { title : 'The Matrix', year : '1999-03-31' })
CREATE (matrix2:Movie { title : 'The Matrix Reloaded', year : '2003-05-07' })
CREATE (matrix3:Movie { title : 'The Matrix Revolutions', year : '2003-10-27' })
CREATE (keanu:Actor { name:'Keanu Reeves' })
CREATE (laurence:Actor { name:'Laurence Fishburne' })
CREATE (carrieanne:Actor { name:'Carrie-Anne Moss' })
CREATE (keanu)-[:ACTS_IN { role : 'Neo' }]->(matrix1)
CREATE (keanu)-[:ACTS_IN { role : 'Neo' }]->(matrix2)
CREATE (keanu)-[:ACTS_IN { role : 'Neo' }]->(matrix3)
CREATE (laurence)-[:ACTS_IN { role : 'Morpheus' }]->(matrix1)
CREATE (laurence)-[:ACTS_IN { role : 'Morpheus' }]->(matrix2)
CREATE (laurence)-[:ACTS_IN { role : 'Morpheus' }]->(matrix3)
CREATE (carrieanne)-[:ACTS_IN { role : 'Trinity' }]->(matrix1)
CREATE (carrieanne)-[:ACTS_IN { role : 'Trinity' }]->(matrix2)
CREATE (carrieanne)-[:ACTS_IN { role : 'Trinity' }]->(matrix3)

\end{lstlisting}
\vspace*{\fill}
\newpage



\subsection*{Cassandra}
TO BE COMPLETE












