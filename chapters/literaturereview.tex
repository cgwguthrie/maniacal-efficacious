\chapter{Literature Review}
The purpose of this project is to identify and analyse the functional capabilities and running performance of a number of leading NoSQL solutions. This report will assist the reader in understanding the limitations of each of the solutions and provide a level of comprehension, proficient enough to support reasoning in choosing to use one solution versus another. This literature review enhances the theoretical understanding of the project and influenced many of the decisions made throughout the project as a result.
\section{Big Data Defined}\label{bigdata}
Big Data is a broad, evolving term bound to a complex and powerful application of analytical insight which over recent years has had a variety of definitions. In simplistic terms Big Data can be described as extremely large datasets that may be studied computationally to reveal patterns, trends, and associations for ongoing discovery and analysis.

The 2011 McKinsey Global Institute (MGI), a multinational management consultancy firm, compiled a report namely ``Big data: The next frontier for innovation, competition, and productivity" outlines the potential effects big data will have on a number of industries. The report suggests that with the increasing ``exponential" growth of data volume, simply recruiting a ``few data-orientated managers" will be a temporary fix rather than a lasting solution. MGI suggest that if companies in a variety of sectors, such as the healthcare and retail industry, were to take advantage of the value which big data brings could see potentially huge returns. ``...a retailer using big data to the full could increase its operating margin by more than 60 percent" \cite{mckinskey}. The report also states that if ``healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than \$300 billion in value every year" \cite{mckinskey}. Thus further cementing the view which accepts that big data plays a pivotal role in everyday modern life.

\subsection{5vs Model}\label{5vs}
In 2001, Gartner analyst Doug Laney delivered the \textit{3vs model} which defines the challenges and opportunities which have arisen from the increase in data volume. Laney categorises big data into three dimensions; Volume, Velocity and Variety, with the increase of each encapsulating the challenges currently faced today of big data management. \begin{figure}[h]\begin{center}\includegraphics[width=0.8\linewidth]{images/3vs}\caption{3vs data model}\label{fig:3vs}\end{center}\end{figure}
\parindent 0pt
The characteristics of each property illustrated in figure \ref{fig:3vs} are defined as: \textbf{Volume} - The vast amounts of data generated every second. With the creation and storage of large quantities of data, the scale of this data becomes progressively vast. \textbf{Velocity} -  The speed at which new data is generated and the speed at which data moves around emphasising the ``timeliness" of the big data. In order to fully profit from the commercial value of big data, data collection and data examination must be conducted promptly . \textbf{Variety} - This characteristic alludes to the various types of data we can now use; semi-structured and unstructured. Examples being ``audio, video, webpage and text as well as traditional structured data" \cite{bigdata}.

\parindent 15pt Big Data is a term becoming increasingly common in business and society. Overcoming obstacles and implementing effective, actionable Big Data strategies is key for successful big data management. In recent years a fourth category was introduced; \textbf{Veracity} - Data inconsistencies and incompleteness result in data uncertainty and unreliability; which creates a new challenge, keeping data organised \cite{bigdata}.

The final, and considered by many to be the most important \textit{V} of big data, is \textbf{Value}. ``All the volumes of fast-moving data of different variety and veracity have to be turned into value" \cite{ibm}. One of the biggest challenges faced by organisations is having the ability to turn data into something useful. It can be an easy trap to fall into for a business aiming to embark on big data initiatives without a clear understanding of costs and benefits \cite{bigdata}. Thus the importance of establishing clear and achievable business objectives.

\begin{figure}[h]\begin{center}\includegraphics[height = 8cm,width=1\linewidth]{images/worldpopgraph}\caption{World population vs Internet users}\label{fig:worldpop}\end{center}\end{figure}
The amount of data produced has dramatically increased from when Laney first introduced the 3vs model in 2001. This is in no small part due to the availability and accessibility of the internet. In 1995 the internet had on average 45 million users, 1\% of the worlds population. This figure increased to over 1 billion people with internet access worldwide in 2005, and by 2010 nearly 2 billion which was 30\% of the worlds population. The latest figures show that in 2015 the penetration of the internet reached 3 billion people, 40\% of the entire population. Social media sites such as Facebook, Twitter, Snapchat, Instagram and Pinterest, are some of the main contributors in generating large volumes of user data. Facebook boast a staggering, 1 million links shared, 2 million friend requests and 3 million messages sent on average every twenty minutes \cite{statref}. The graph and data table in figure \ref{fig:worldpop} illustrate the continual growth of internet accessibility as a whole.

\section{Extract Transform Load}\label{etlprocess}
This project will require the extraction, manipulation and processing of a data source from one data model to another. This process is commonly known as Extract Transform Load (ETL). Section \ref{etlprocess} discusses each stage involved in the ETL procedure followed by section \ref{etltool} which examines the ETL implementation methodology and its relevance to this project. 

A basic definition of the Extract Transform Load (ETL) process is pulling data from one database, refactoring the composition of the data and putting the data into another database. While the name ETL implies there are 3 main categorisation stages - extract, transform, load - the procedure in its entirety is a much broader and expansive process which encompass these stages. Despite this the procedure is split in to these three stages. Figure \ref{fig:etl} illustrates the ETL process with data coming from a source; a file or database management system for example then being transformed in to the required format for a successful load. \begin{figure}[h]\begin{center}\includegraphics[width=0.8\linewidth]{images/etl.jpg}\caption{ETL process}\label{fig:etl}\end{center}\end{figure}

\subsection{Extract}
Extract is the first step in the ETL procedure in which data is read from a source system, usually a database but not restricted to, and makes it available for processing. The main objective of the extract stage is to retrieve all the required data from a source system using as little resources as possible \cite{etlref1}. It is common for data to be extracted from source systems with different organisations and formats to that of the target system. The extract stage provides an opportunity to \textit{cleanse} the data from the source system as often there will be redundant or irrelevant data which is not required.

\subsection{Transform}
Transform is where the extracted data is manipulated from its previous state and converted into a target system format. The step involves the application of a set of rules or functions to transform the data from the source to the target. As well as the applied rules and functions the transformation step is responsible for the validation of records ensuring unacceptable records are removed accordingly. ``The most common processes used for transformation are conversion, clearing the duplicates, standardizing, filtering, sorting, translating and looking up or verifying if the data sources are inconsistent." \cite{etlref2}.

\subsection{Load}
Load completes the three step procedure and is where data is written into the target system. There are multiple ways in which data is loaded into a system using the ELT methodology. One of which and the most obvious is to physically insert the data. For example if the target repository is a SQL database insert the data as a new row using the relevant \textit{Insert} statement. An alternative to loading the process manually is that some ETL tool implementations have the capability to ``...link the extraction, transformation, and loading processes for each record from the source." \cite{etlref2}. Depending on the technique applied the load step of the process can become the most time consuming.

\section{Software Tools}\label{softwaretools}

This chapter discusses the tools which were used in the project as part of the ETL process. It focuses on the lesser known software packages such as Google Refine in section \ref{grefine} and

\section{Knowledge Representation Languages}
A Knowledge Representation (KR) language can be defined as: for any given interpretation of a sentence or string of text the KR must have the ability to effectively and unambiguously express knowledge in a both a human and computer manageable form. 

There are a number of options and possibilities for communicating data and information which range from binary representation to meta markup languages such as Extensible Markup Language (XML) for example. Markup languages which are easily read by humans such as XML which as a result of its rigid set of rules lends its self to both humans and machines. Comparatively binary notation which uses 1's and 0's to represent data, while relatively cheap in terms of computing power the ability to comprehend this notation requires a unique and specific skill set.

\subsection{Semantic Web}\label{semanticweb}
The Semantic Web has two main intended outcomes. The first is about the standardised formats of data pulled from variety of sources, whereas the original Web concentrated on the interchange of documents \cite{semantic}. The second outcome of the Semantic Web is the language for recording the relation between data and objects in the real world. ``That allows a person, or a machine, to start off in one database, and then move through an unending set of databases which are connected not by wires but by being about the same thing."  \cite{semantic}.  The Semantic Web is about taking information which is already on the Web and making it meaningful to computers. It is the next major step in connecting information on a global scale. Web pages are documents designed to be humanly readable. The aim of the Semantic Web is to take these documents and by extending the principles of the Web,transform them into data, readable by machines.The term was originally coined by Tim Berners-Lee, the inventor of the World Wide Web.Berners-Lee became frustrated with current state of the Web. He believed that the Web as a whole should be more intelligent about the way it serves user?s needs. W3C, the organisation who define web standards and founded by Berners-Lee, describe the Semantic Web as ``...a common framework that allows data to be shared and reused across application, enterprise,and community boundaries'' \cite{13} While search engines index the majority of the Web?s content to find what a user thinks they are looking for, their ability to select pages which a user truly wants or needs is lacking. This is because we do not have a web of data.Figure 1: Semantic Web Layer Cake \cite{12}. Figure \ref{fig:smw} represents the so called Semantic Web layer cake,created by Berners-Lee. It illustrates the architectural hierarchy of languages used for the SemanticWeb. \begin{wrapfigure}{r}{}\includegraphics[width=0.5\linewidth]{images/semanticwebcake}\caption{Semantic web layer cake}\label{fig:smw}\end{wrapfigure} Each layer, from top to bottom,uses the capabilities of the layers below. The top 3 layers Trust,Proof and Logic are technologies which are not yet standardised and are just ideas that should be implemented in order to realise SemanticWeb. The middle layers Ontology Vocabulary and RDF + rdfschema are standardised technologies by W3C which enable the building of Semantic Web applications.The bottom layers contain technologies that are well known from hypertext web and that without change provide basis for the Semantic Web.There are a number of technologies for the Semantic Web. The main three are RDF,SPARQL and OWL. The Ontology Vocabulary layer of the Semantic Web stack is one of the most important. Renowned Computer Scientist, Tom Gruber?s, simple definition of an ontology is often cited by authors ``An ontology is an explicit specification of a conceptualization''. The Web Ontology Language OWL is a language representation standard for designing and authoring Web ontologies produced by W3C. \cite{owl}. The OWL file format is designed to be used by applications required to process the content of the information and to be humanly readable. ``[OWL] is intended to provide a language that can be used to describe the classes and relations between them that are inherent in Web documents and applications."\cite{owl}. The OWL languages are characterised by formal semantics and are built upon the W3C standard RDF format. While there is no universally accepted definition of an ontology, they are considered as one of the main pillars of the Semantic Web. A Vocabulary is often described as a special form of ontology or simply as a collection of URIs \cite{11}. A UniformResource Identifier (URI) is is the way one identifies a point of content. For example a page of text, a video or sound clip, a still or animated image, or a program. The OWL language is3characterised by formal semantics and is built upon a W3C XML standard for objects known as Resource Description Framework (RDF) \cite{15}. There are many fields which have shown significant interest into OWL and RDF specifically academic, medical and commercial circles.RDF is part of the W3C family originally designed as a metadata data model. It is a general framework allowing one to describe Internet resources, for example a website, and it?s content.One of the main concepts of RDF is n RDF triple. A triple consists of:

\begin{itemize}
\item \textbf{Subject} - A RDF URI reference or a blank node.
\item \textbf{Predicate} - A RDF URI reference.
\item \textbf{Object} - A RDF URI reference, a literal or a blank node.
\end{itemize}

The aim of RDF is to enable people to share website and other descriptions with ease. Consequently giving software engineers the opportunity to develop more intelligent search engines and directories. Thus giving Web users more control of what they are browsing on the Web. A typical RDF description can include data such as resource author, date of creation/updated, subject categories and information summarising the content for audience. The way in which an RDF document is queried is predominately by using a query language called SPARQL. SPARQL is a query language designed to retrieve and manipulate data stored in a RDF structure. It became a standardised technology in 2008 by W3C and remains to be one of the key technologies of the Semantic Web. A standard SPARQL query consists of a set of triples where the subject, predicate and/or object can consist of variables \cite{7}. The way in which a SPARQL query works is by matching the triples in the SPARQL query with the existing RDF triples and find solutions to the variables.

\section{NoSQL}\label{nosql}
NoSQL is labeled as a next generation database known to most as ``Not only SQL" \cite{nosql1}. This definition however insinuates its defiance against the industry standard SQL. It was originally developed in 1998 by Carlo Strozzi; a member of the Italian Linux society, with the intention of being a non-relational, widely distributable and highly scalable database. Strozzi named the database management system NoSQL to merely state it does not express queries in the traditional SQL format. Sadalage and Fowler believe the definition we commonly refer NoSQL as comes from a 2009 conference in San Fransisco held by Johan Oskarsson, a software developer. Sadalage and Fowler recall Oskarssons desire to generate publicity surrounding the event and in an attempt to do so devised the twitter hashtag ``NoSQL Meetup". The main attendees at the conference debrief session were Cassandra, CouchDB, HBase and MongoDB and so the association stuck. \cite{nosql1}

NoSQL solutions are not bound by a definitive schema structure. This permits the ability to freely adapt database records or add custom fields for example without considering structural changes. This is extremely effective when dealing with varying data types and data sets, in comparison to the traditional relational database model which when tackling this issue often resulted in ambiguous field names.  \cite{nosql1}

\subsection{Distributed Systems and Data Transactions}\label{distrosystems}
\subsubsection{Brewer's CAP Theroem}\label{captheroem}
\subsubsection{ACID Transactions}\label{acid}
\subsubsection{Distributed Database}\label{distributeddb}
A distributed database (DDB) comprises of two or more data files located at different sites and servers on a computer network. \cite{dd} The advantage of using a DD is that as the database is distributed, multiple users can access a portion of the database at different locations locally and remotely without obstructing one another's work. It is  pivotal for the DD database management system to periodically synchronise the scattered databases to make sure that they all have consistent data.  \cite{dd} For example if a user updates or deletes data in one location is is essential this change is mirrored on all databases. This ability to remotely access a database from all across the world lends itself to not only multinational companies for example but also startup businesses which recruit the expertise of others from various locations.

\subsection{Database Classification}\label{dbclass}
One of the first decisions to be made when selecting a database is the characteristics of the data you are looking to . \cite{nosql2} There are a multitude of options available with many different classifications. The following sections discuss a subset of these which are relevant to this project.

\subsubsection{Document-Oriented Database}
Document-orientated database (DODB) are designed for storing, retrieving and managing document files such as XML, JSON and BSON. The documents stored in a DODB model are data objects which describe the data in the document, as well as the data itself. \begin{figure}[h]\begin{center}\includegraphics[width=0.75\linewidth]{images/mongodbmodel}\caption{MongoDB document}\label{fig:mongo}\end{center}\end{figure} Figure \ref{fig:mongo} illustrates an example document stored in a DODB specifically in MongoDB. The data is a recognisable JSON format and the joins of the document are between common variable values within each document.

\subsubsection{Graph-Orientated Database}
A graph-oriented database (GODB), is a form of NoSQL database solution that uses graph theory to store, map and query relationships. A graph is a collection of nodes connected by relationships. ``Graphs represent entities as nodes and the ways in which those entities relate to the world as relationships."  \cite{gd} The formation of the graph database structure is extremely useful and eloquent as it permits clear modelling of a vast and often ecliptic array of data types.  \cite{gd} An example of data represented in a graph structure is the Twitter relationship model. \begin{figure}[h]\begin{center}\includegraphics[width=0.5\linewidth]{images/graphdb_twitter}\caption{Example tweet data relationship}\label{fig:twitter}\end{center}\end{figure} Figure \ref{fig:twitter} illustrates the nodes involved in a standard tweet and the relationship link between them. The labeled nodes indicate the various operations which are involved in one the tweet. One interpretation of the figure \ref{fig:twitter} example is that a user posts a tweet, using the Twitter App which mentions another user and includes a hashtag and link.

\subsubsection{Column-Orientated Database}
A column-orientated database (CODB) is a database management system that stores data tables as columns of data rather than as rows of data. The main objective of a CODB is to write and read data from the hard disk efficiently in an attempt to speed up querying time. A CODB has the ability to self index which uses less disk space than RDBMS which holds the same data. A CODB can also be highly compressed, resulting in aggregate functions such as MIN, MAX and SUM to be performed at a extremely high rate. \cite{cd}.

\begin{figure}[h]\begin{center}\includegraphics[width=1\linewidth]{images/codb}\caption{Column orientated database example}\label{fig:cod}\end{center}\end{figure}

Figure \ref{fig:cod} illustrates the comparison of a RDB model against a CODB model. Within the row based model the data contains both multiple values per record and null values. However in the CODB model null values are not required as each record contains a minimum and maximum of one value.

\section{Relational Database}
A relational database (RDB) is a collection of data items organised as a set of tables, records and columns from which data can be accessed or reassembled in many different ways \cite{rdb}. The connected tables are known as relations and contain one or more columns which comprise of data records called rows. Relations can also be instantiated between the data rows to form functional dependencies.

\begin{itemize}
\item One to One: One table record relates to another record in another table.
\item One to Many: One table record relates to many records in another table.
\item Many to One: More than one table record relates to another table record.
\item Many to Many: More than one table record relates to more than one record in another table.
\end{itemize}


\section{Technology Evaluation}\label{techeval}
The technologies being evaluated in this project are outlined below. \textbf{ DISCUSS QUERY LANGUAGE!}

\subsection{MySQL}\label{mysql}
MySQL is a freely available open source RDB that uses Structured Query Language (SQL). MySQL is commonly used for web applications with its speed and reliability being a key feature. The MySQL database stores data in tables - a collection of related data - which consists of columns and rows. MySQL runs as a server and allows multiple users to manage and create numerous databases. 

SQL is a programming language used to communicate with databases through queries. SQL queries are used to perform tasks such as update or retrieve data in a database. The queries are in the form of command line language which include keyword statements such as select, insert and update.

\subsection{MongoDB}\label{mongo}
MongoDB is an open source cross-platform DODB. The premise for using MongoDB is simplicity, speed and scalability  \cite{md}. Its ever growing popularity, specifically amongst programmers, stems from the unrestrictive and flexible DODB data model which gives you the ability to query on all fields and boasts instinctive mapping of objects in modern programming languages. \cite{md} The database design of MongoDB is based on the JSON file format named BSON. 

A record in MongoDB is known as a document; a data structure composed of field and value pairs. The values of fields can include other documents, arrays and arrays of other documents. The key features of using MongoDB are its high performance data persistence, provide high availability and automatic scaling  \cite{md}.

\subsection{Neo4j}\label{neo}
Neo4j is an open-source NoSQL GODB which imposes the Property Graph Model throughout its implementation. The team behind the development of Neo4j describe it as an ``An intuitive approach to data problems" \cite{ndweb}. One of the reasons in which Neo4j is favoured predominantly amongst database administrators and developers is its efficiency and high scalability. This is in part due to its compact storage and memory caching for the graphs. ``Neo4j scales up and out, supporting tens of billions of nodes and relationships, and hundreds of thousands of ACID transactions per second." \cite{ndweb}

The key features of Neo4j which lends itself to users, developers and database administrators are its ability to establish relationships on creating, the equality of relationships permits the addition of new relationships being created after initial implementation at no performance cost and its use of memory caching for graphs which allows efficient scaling.

\subsection{Apache Cassandra}\label{cassandra}
Apache Cassandra is an open source column-orientated DDB that is designed for storing and managing vast amounts of data across multiple servers. ``Apache Cassandra is a highly scalable, high-performance distributed database designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure." \cite{cassandra}. Apache Cassandra define the key features of their database management system as ``continuous availability, linear scale performance, operational simplicity and easy data distribution across multiple data centres and cloud availability zones." \cite{cassandra}.  Figure \ref{fig:cass} illustrates an example record stored in a Cassandra database. \begin{figure}[h]\begin{center}\includegraphics[width=0.70\linewidth]{images/cassandramodel}\caption{Example Cassandra record}\label{fig:cass}\end{center}\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{CQL Type}} & \multicolumn{1}{c|}{\textbf{Constants}} & \multicolumn{1}{c|}{\textbf{Description}} \\ \hline
ascii & strings & US-ASCII character string \\ \hline
bigint & integers & 64-bit signed long \\ \hline
blob & blobs & Arbitrary bytes (no validation), expressed as hexadecimal \\ \hline
boolean & booleans & true or false \\ \hline
counter & integers & Distributed counter value (64-bit long) \\ \hline
decimal & integers, floats & Variable-precision decimal \\ \hline
double & integers & 64-bit IEEE-754 floating point \\ \hline
float & integers, floats & 32-bit IEEE-754 floating point \\ \hline
inet & strings & IP address string in IPv4 or IPv6 format* \\ \hline
int & integers & 32-bit signed integer \\ \hline
list & n/a & A collection of one or more ordered elements \\ \hline
map & n/a & A JSON-style array of literals: \{ literal : literal, literal : literal ... \} \\ \hline
set & n/a & A collection of one or more elements \\ \hline
text & strings & UTF-8 encoded string \\ \hline
timestamp & integers, strings & Date plus time, encoded as 8 bytes since epoch \\ \hline
uuid & uuids & A UUID in standard UUID format \\ \hline
timeuuid & uuids & Type 1 UUID only (CQL 3) \\ \hline
varchar & strings & UTF-8 encoded string \\ \hline
varint & integers & Arbitrary-precision integer \\ \hline
\end{tabular}
\caption{Cassandra data types}
\label{tab:cassdt}
\end{table}

\section{Data Source and Representation}\label{datasource}
The dataset used as a resource to populate the database solutions is called EMAP; a freely available anatomical ontology of the developmental stages of mouse embryos. The EMAP dataset was chosen for this project as my supervisor has much experience in the field and would be able to assist me with any queries I had regarding the data. The size and granularity of the EMAP dataset also meets the criteria which will be required to test the database solutions, explore the limitations of each database comparatively and pose insight into the overall performance of each database.

\subsection{EMAP}
The \textit{Edinburgh Mouse Atlas Project} (EMAP) is an ongoing research project to develop a digital atlas of mouse development. The objective of the EMAP is to implement a digital model of mouse embryos for each time stage in development \cite{emap}. The collated model embryo data is then used to form a database from which further research can be conducted and experiments can be mapped.
 
Each time step in the digital model are named \textit{Theiler Stages} inspired by the research conducted by Karl Theiler. A Theiler Stage defines the development of a mouse embryo by the form and structure of organisms and their specific anatomical structural features. There are 26 individual Theiler Stages which define the growth and evolution of the mouse embryo. The Theiler Stage scheme comprises of both the anatomical developmental stage definition and the estimated length of time since conception. Each Theiler Stage also provides a brief description of the anatomy and any significant changes between the current and previous stage.

Theiler proposed using this scheme as embryos at the same developmental age can have evolved at different rates and therefore exhibit different structural characteristics  EMAP has developed a collection of three dimensional computer models which illustrate and summarise each Theiler Stage.\cite{emap} 

The anatomy generated at each Theiler Stage has an associated ontological representation. Each provides an alternative aspect of the evolution of a mouse embryo which corresponds with a respective Theiler Stage. The abbreviated term EMAP carries a certain amount of ambiguity as it is refers to the name of the project, and one of the stages in the implemented anatomy. For the purpose of this project the main anatomy to be utilised is the aggregated non stage specific Edinburgh Mouse Atlas Project Abstract (EMAPA) anatomy.

\subsection{EMAP anatomy}
The EMAP anatomy ontology was originally developed to deliver a stage-specific anatomical structure for the developing laboratory mouse. As the EMAP research has progressed, the ontology has followed suit, and is continually under development.

The original EMAP anatomy ontology consists of a series of relational components organised in a hierarchical tree structure which utilise ``part-of" relations and subdivisions which encompass each Theiler Stage \cite{emap}. The intention behind the implementation of the original ontology structure was to ``...describe the whole embryo as a tree of anatomical structures successively divided into non-overlapping named parts" \cite{emap}.

Each of the Theiler Stage components has an appropriately named term label, known as the \textit{short name} which describes each respective component. Each Theiler Stage also has a \textit{full name} which comes in the form of the components entire hierarchical path \cite{emap}. Neither the \textit{full} nor \textit{short} anatomical name of each component are required to be distinct and can appear in several Theiler Stages. Therefore to avoid ambiguity each component can be addressed by a unique identifier. The unique identifier is in the form of the relevant anatomy followed by a number (EMAP:number). For example ``choroid plexus" is the short name of TS20/mouse/body region/head/brain/choroid plexus and has a has unique identifier of EMAP:4218. \begin{figure}[h]\begin{center}\includegraphics[width=1\linewidth]{images/emapachoroidplexus}\caption{EMAPA data structure}\label{fig:emapa}\end{center}\end{figure}

The EMAP hierarchical structure facilitates the need for basic ``data annotation and integration" however a combination of the lack of hierarchical views, missing or poorly represented Theiler Stages and label name ambiguity exposed the limitations of the EMAP structure. As a result the need for a hybrid ``abstract" version of the anatomy was identified and subsequently developed; EMAPA. \cite{emap} Thus the EMAPA anatomy will be the main data source for this work.

The research surrounding the EMAP resource is continually being developed, thus the growth of the project as a whole is progressively increasing with the richness of data at the heart \cite{emap}. The EMAPA ontology discussed below in section \ref{emapaanatomy} is now considered the primary data source thus the EMAP dataset is available in a combined EMAP and EMAPA standard ontological format developed by the Open Biological Ontologies (OBO) consortium (Section \ref{obo}).

\subsection{EMAPA anatomy}\label{emapaanatomy}
EMAPA is a refined and algorithmically developed non-stage specific anatomical ontology \textit{abstract} representation of the EMAP anatomy. The EMAPA implementation replaces the EMAP hierarchical tree structure for a \textit{directed acyclic graph} structure; a graph in which it is impossible to start at some vertex v and follow a sequence of edges that eventually loops back to v again. Thus enabling the ability to represent multiple parental relationships and other forms of ``is-a" relations where appropriate \cite{emap}.

Each anatomical component in the EMAPA anatomy is identified as a single term, coupled with the appropriate start and end Theiler Stage at which the component is considered to be present in the developing embryo. \cite{emap} With the aim of enhancing user experience, the EMAPA anatomy  implements an alternative naming convention from the EMAP anatomy replacing full path names for components to \textit{``print names"}. Using the above example for comparison, ``EMAP:4218" in the EMAP anatomy becomes ``TS20 brain choroid plexus" in EMAPA. This naming convention supplements the requirement of uniqueness and is easily comprehensible.

The EMAPA ontology is available in a standard ontological format developed by the Open Biological Ontologies (OBO) consortium (Section \ref{obo}) and is also available in a Web Ontology Language (OWL); a standard produced from W3C discussed in section \ref{owl}.This enhanced version of the ontology EMAPA, is now considered to be the primary EMAP anatomy ontology thus will be the main source for the work on this project.

\subsection{EMAGE anatomy}
EMAGE is a database consisting primarily of image data of \textit{in situ} gene expression data of the developing mouse embryo. The data is sourced from in the community and which is then taken by curators who monitor the EMAGE project and implement it in a standardised way that allows data query and exchange. The description includes a text-based component but the unique aspect of EMAGE is its spatial annotation focus \cite{emap}.

``Sites of gene/enhancer expression in EMAGE are described by denoting appropriate regions in the EMAP virtual embryos where expression is detected (and not detected) and also describing this information with an accompanying text-based description, which is achieved by referring to appropriate terms in the anatomy ontology" \cite{emap}

The EMAGE anatomy provides an alternative view of the EMAP anatomical ontology. The main data source for this project will be EMAPA however should the need for a data set which holds larger data then the EMAGE data set will be used.