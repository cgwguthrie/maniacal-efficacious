\chapter{Database Modelling}\label{design}

In order for me to design a complete database model for each of the technologies, an initial investigation into the specific dataset values was required discussed in section \ref{datasetvalues}. Once this was complete I followed the same database design process for each indexing solution which is discussed in section \ref{dbdesign}.

\section{Cleansing the data}\label{datasetvalues}

The EMAGE data which was supplied was made up of 4 tab separated files. Each file contained information pertaining a certain aspect of the dataset; Annotations, Publications, Submissions and Results.

\begin{itemize}
\item \textbf{Annotations} - Data such as the EMAPA structure ID, the EMAPA structure term, the Theiler Stage, the EMAGE structure ID and the strength in which the each gene was detected.

\item \textbf{Publications} - Data regarding all authored publications for the EMAGE dataset. Data included the title, author(s), Theiler Stage and EMAGE ID for the genes.

\item \textbf{Submissions} - Information on each EMAGE assay. Data such as EMAGE ID, Theiler Stage, probe ID, type of assay (in situ or part of), specimen type and specimen strain.

\item \textbf{Results} - Results from each gene expression. Much of the data in this file was a replicated in the submissions file. Data included Theiler Stage, EMAGE ID, data source, assay type and gene name.
\end{itemize}

On initial review each of the files contained similar, repetitive, meta-data values. Thus creating a level of noise which would not add anything to the project in terms of analysis and evaluation. Therefore I undertook an initial cleansing of the data before implementing the database design.

The cleansing process consisted of loading the data into Open Refine (OR) and manually manipulating the data using the filtering and editing tools the package provides. Using OR allowed me to identify any erroneous rows of data which would affect the integrity of the dataset. In each file there was at most 5 rows of data which were either blank or inconsistent with the convention of the rest of the file. I decided to remove these rows as there inclusion in the file was unnecessary.

Another irregularity found in the \textit{Publications} data file was the characters used in the title and author fields. There were over 600 rows of data which contained a non-ascii character. For example - ten Berge D, Brouwer A, el Bahi S, GuÃ©net JL, Robert B, Meijlink F. These rows would be rejected when importing into the databases as by default I decided to apply a UTF-8 character encoding to each indexing solution. Despite these values only contributing to around 5\% of the file, the issue needed to be addressed. To do this I devised a regular expression which would identify and subsequently remove any of these characters.

Using a software tool such as OR enabled me to manipulate and cleanse the data meaning I would be able to load it successfully into the databases. Despite cleansing the datasets successfully, I identified some potential problematic circumstances. While the identified circumstances may not be as prevalent using the EMAGE dataset, they may affect other big data sets.

The maximum number of rows uploaded into OR was around 150,000. The EMAGE dataset is relatively small in comparison to large scale data collation, however the volume of data was a factor in my decision to use a software tool in an ETL workflow as opposed to rolling my own scripting solution. It is important when choosing a methodology or tool to enhance the veracity of a dataset that the volume of data is taken into consideration. OR is a Java application that utilises the Java Virtual Machine (JVM) and therefore it is integral to allocate enough memory to handle processing large files and thus avoid Java heap space errors. The OR developers suggest that a typical best practice is ``start with no more than 50\% of your available physical memory, since certain OS's will utilize up to 1 Gig or more of physical RAM." \cite{googref}. While using this software solution was sufficient for the data in this project, should the dataset be of a greater scale, a more robust and resilient system would need to be considered.

As discussed in section \ref{5vs} a major challenge in data collection and manipulation is ensuring the veracity of your data. A leading contributor to this challenge is human error. It is a fact of life that humans are error prone and can often make mistakes, therefore where possible the minimal amount of manual handling of a dataset is key. An example of an issue which can arise from this may be as simple as date formatting changing over time. The data may initially be input in a UK standard date format of DD-MM-YYYY by one person and then stored in a US standard data format of MM-DD-YYYY by another person. A simple example, however one which can have serious repercussions on the validity of a dataset. The cleansing of the EMAGE dataset relied on my knowledge of the data and any obvious flaws such as blank values where a value was required. While the data provided was reliable and generally healthy; a richer more granular dataset may require a more rigorous method of validation. One way to do this would be to implement a software script which takes a subset of the data, defines a format, and restructures the remaining data in the dataset accordingly.

\section{Designing the data models}\label{dbdesign}

In order for me to develop multiple and reliable data models which accurately represent the dataset, I created a database diagram for each indexing solution. Each diagram effectively illustrates the relationship between the data entities. The order in which I decided to create the database model designs was based upon two main reasons; which hinge upon aiding the reader's comprehension of this thesis. The first reason was based upon my previous experience of using each of the database systems. My previous experience ranged from a competent level to the complete unknown. Secondly, as MySQL is a well known database management system, and is widely used for a number of applications, it is expected that the reader will already have a functioning knowledge of the system. As a result I decided the first data model I would develop would be for the relational database management system, MySQL. The next system I developed was MongoDB, followed by Neo4j and finally Apache Cassandra. Each implementation presented a different challenge all of which will be discussed below.

\subsection{MySQL - data model design}\label{mysqldesign}

As discussed in section \ref{mysql} MySQL is a relational database management system which stores and represents structured data through entity tables and relationships. There are a number of variations in which the design of the MySQL database could be modelled for the EMAGE data. Figure \ref{fig:mysql} is an entity-relationship (ER) diagram which illustrates the implementation of my MySQL normalised database design. Normalisation in database design is a process by which an existing schema is modified to bring its component tables into compliance through a series of progressive normal forms. It aids in better, faster, stronger searches as it entails fewer entities to scan in comparison with the earlier searches based on mixed entities. Data integrity is improved through database normalisation as it splits all the data into individual entities yet building strong linkages with the related data. The below description provides and overview into each table and its entities.

\begin{itemize}
\item \textbf{AnatomyStructures} : This table contains the EMAPA ID, and the term which refers to the part of the anatomy.
\begin{itemize}
\item Many to One relationship with the Stages table as one anatomy structure can have the same stage.
\item Many to One relationship with itself on the ID as one structure can have many parts.
\end{itemize} 

\item \textbf{Assays} : This table contains the EMAGE ID number, the ID of the probe and the type of assay. The type field refers to whether the assay is ``in situ'' or ``part of''.
\begin{itemize}
\item Many to One relationship with the Sources table. While one assay can only have one source, many assays can have the same source.
\end{itemize} 

\item \textbf{Genes} : This table contains the accession number and symbol of each gene.
\begin{itemize}
\item The Genes table does not reference any other table.
\end{itemize} 

\item \textbf{Publications} : This table contains the accession, title and every author of each assay publication.
\begin{itemize}
\item Many to One relationship with the Assays table as there can be many publications for one assay.
\end{itemize} 

\item \textbf{Sources} : This table contains the source of the assay.
\begin{itemize}
\item The Sources table does not reference any other table.
\end{itemize} 

\item \textbf{Specimens} :  This table contains the ID, strain and type of each specimen. The type field refers to whether the assay is a section, wholemount, sectioned wholemount or unknown.
\begin{itemize}
\item One to One relationship with the Assays table as one assay has one specimen.
\item Many to One relationship with the Stages table as many specimens can have the same Stage.
\end{itemize} 

\item \textbf{Stages} : This table contains the theiler stage and number of days post conception (dpc) of each assay, specimen and anatomy.
\begin{itemize}
\item Many to One relationship with the Assays table as one assay has can have multiple stages.
\end{itemize} 

\item \textbf{TextAnnotations} : This table contains the structure and strength of each assay.
\begin{itemize}
\item Many to One relationship with the Assays table as one assay can have multiple text annotations.
\item Many to One relationship with the Genes table as one text annotation can have multiple genes.
\item One to One relationship with the AnatomyStructures table as one text annotation can have one structure.
\end{itemize} 

\end{itemize}

\newpage
\begin{figure}[H]\begin{center}\includegraphics[width=1\linewidth]{images/emage_erd}\caption{MySQL ER table diagram}\label{fig:mysql}\end{center}\end{figure}

\subsection{MongoDB  - data model design}\label{mongodesign}

As discussed in section \ref{mongo}, MongoDB is a homogeneous, schema-less, NoSQL document store database. There are no formal relations between the data which makes modelling the database a little more challenging; especially with data which is so closely bound as the EMAGE dataset.

Data in MongoDB sits in \textit{collections}, a grouping of documents which are stored on a database. A collection exists within a single database and is the equivalent of an RDBMS table. Documents within a collection can have different fields and typically all documents in a collection have a similar or related purpose. 

The MongoDB implementation was the second prototype data model I created for this project. I followed the same structural process which I had undertaken for the previous MySQL data model. When creating a MongoDB data model there are a number of factors and considerations which need to be identified before starting the formal implementation. Firstly the biggest decision I deliberated over was how should the data be connected. As there were a few options I decided to explore all of them to fully comprehend the pros/cons of each.

My initial design was based around using multiple collections to store the various aspects of the data. The design followed the MySQL model, with 4 collections; Assays, Text Annotations, Anatomy Structures and Genes. To connect the data and bind the values, required an additional manually developed ID field for every document. While this was not a complex task, it was one which I felt was unnecessary and added extra unwanted noise to the data. Using this option would have also incurred more overhead when writing the queries for the database. One would firstly have to connect the data (similar to a RDBMS join) and then include a further query. This can only be done at the application level as opposed to querying directly with the database. Depending on the query and number of collection joins, this may not be overly time consuming. Nevertheless, additional resource is still required. 

After some deliberation I concluded that the best way to implement the data model would be to have all of the data in the one collection. Figure \ref{fig:mongo} illustrates an example document in the developed MongoDB data model. The diagram should be read as follows:

\begin{itemize}
\item \textbf{id} : This is the EMAGE id of each assay and is the value which binds all of the data together. The id value has been manually configured to correspond with the EMAGE value.
\item \textbf{specimen} : The specimen value is an array of size 2 which holds data regarding the strain and type of the assay.
\item \textbf{probe id} : This value is the id of the probe accession.
\item \textbf{assay source} : The source in which the assay has been retrieved from.
\item \textbf{assay type} : The type of assay which is being analysed. By type I am referring to whether the assay is \textit{in situ} or otherwise.
\item \textbf{stage} : An array containing the information regarding the stage of the assay; Theiler stage and DPC.
\item \textbf{publication} : An array containing all publication information regarding that specific assay; id, title, author.
\item \textbf{text annotation} : The text annotation array is the grouping of strength, anatomy structure and gene of an assay. An anatomy structure has a term id and the name of the term and a gene has the symbol and id.
\end{itemize}

\newpage
\begin{figure}[H]\begin{center}\includegraphics[width=1\linewidth]{images/mongo_modeldesign}\caption{Example MongoDB document diagram}\label{fig:mongo}\end{center}\end{figure}

\newpage
\subsection{Neo4j - data model design}\label{neomodel}
Neo4j is a graph orientated, NoSQL database solution. It uses the Property Graph Model methodology of connecting data by nodes and weighted edges. Nodes are the equivalent of a row in a MySQL table and edges are the equivalent of a relation. A full description of Neo4j can be found in section \ref{neo}.

The data model I have constructed for Neo4j, is semantically similar to that of the MySQL implementation. There are 8 nodes, which contain relatively the same data as that of each MySQL table. The main difference between the two systems is how the data is joined. Where MySQL has a foreign key join, Neo4j has an edge, which connects the nodes.

As with MongoDB and indeed many NoSQL system, there is no formal way to diagrammatically convey the database ``schema''. To illustrate the Neo4j data model, I have created two diagrams. The first is an entity relationship (ER) diagram - figure \ref{fig:neo1} - and the second - figure \ref{fig:neo2} - is an example visualisation, aiming to convey the look and feel of the graph model. In terms of the ER diagram, it should be translated as, an entity is a node and a join is a relationship. The data within the nodes is:

\begin{itemize}
\item \textbf{Assay}: This node contains data such as EMAGE ID, probe ID and type e.g. in situ. The Assay node has 4 self-defining relationships.
\begin{itemize}
\item Assay COMES FROM Source
\item Assay HAS Specimen
\item Assay HAS Stage
\end{itemize}
\item \textbf{Publication}: This node contains all publication data, which includes; title, author and id of each assay publication. The Publication node has one relationship.
\begin{itemize}
\item Publication DESCRIBES Assay
\end{itemize}
\item \textbf{Source}: The source node has just one field, source name. It defines the source of each assay.
\item \textbf{Specimen}: Each assay has a specimen node. This node stores the specimen strain and specimen type. It has one relationship, which is the link between the stage node.
\begin{itemize}
\item Specimen HAS Stage
\end{itemize}
\item \textbf{Stage}: The stage node contains each Theiler Stage and DPC value.
\item \textbf{Annotation}: This node contains information regarding the strength of each annotation. It is the join between the Gene and Anatomy Structure nodes. The annotation node has two relationships.
\begin{itemize}
\item Annotation REPORTS Assay
\item Annotation HAS AnatomyStructure
\end{itemize}
\item \textbf{Gene}: This node contains all data regarding each gene found. Data such as; gene name and gene accession. The Gene node has one relationship.
\begin{itemize}
\item Gene HAS Annotation
\end{itemize}
\item \textbf{AnatomyStructure}: This node contains all of the data regarding the respective anatomy structures, such as; structure ID and structure term name
\end{itemize}

\begin{figure}[H]\begin{center}\includegraphics[width=1\linewidth]{images/neo4j_model_er}\caption{Neo4j graph model ER diagram}\label{fig:neo1}\end{center}\end{figure}

\begin{figure}[H]\begin{center}\includegraphics[width=1\linewidth]{images/neo4j_model3}\caption{Example Neo4j graph model diagram}\label{fig:neo2}\end{center}\end{figure}
\newpage
\subsection{Apache Cassandra  - data model design}
The Apache Cassandra data model was perhaps the most difficult of all the database system designs to create. This was due to a number of reasons. One of which was, that I had no previous experience of using a Cassandra database, and therefore had to learn a completely new style of working. Secondly, as discussed in section \ref{cassandra}, one of the main aspects of Cassandra family columns and tables is that they do not accept joins. Resulting in tables being created, aiming to satisfy the any potential queries which may be imposed on the database. The repercussions of this concept is discussed in section \ref{INSERT REF}.

Apache Cassandra is a column based data management system. A key characteristic of a column based data store is that they are extremely powerful and can be reliably used to keep important data of very large sizes. Despite not being \textit{flexible} in terms of what constitutes as data, they are recognised as being highly functional and performant \cite{cassandra}. With this is mind, normalisation of a dataset is in fact not the optimal way of developing a Cassandra data model. It is proposed that \textit{denormalisation} and data duplication within the data model returns the best performing output. This is as a result of Cassandra being optimised to perform a high frequency of writes which in turn reduces the more costly reads. This is a trade-off which must be taken into consideration when developing the data model \cite{cassandra}.




