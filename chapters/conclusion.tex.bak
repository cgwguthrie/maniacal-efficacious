\chapter{Summary and Conclusion}\label{conclusion}
\section{Evaluation}\label{evaluation}
Each of the objectives identified in section \ref{evaluationstrategy} have been achieved. These are outlined below:\\[0.5em]

\textbf{Data Modelling}
\begin{addmargin}[2em]{0pt}
Before designing the data models, I had to firstly cleanse the datasets. The EMAPA dataset required no alterations, therefore I was able to load straight into the databases. Conversely, the EMAGE dataset included invalid non-ascii characters. The databases all had a collation of utf8, thus would not accept these values. As this was a simple task, I used a software package called Open Refine. This tool has much of the same functionality as Excel, and allowed me to filter on specific values. I used a regular expression to identify the values and then fix them as required. When there are data inconsistencies such as these, using a software tool is a viable option to resolve the issue. However, it is often the case that the dataset will contain more technical and complex problems. Therefore, writing a piece of code to cleanse a dataset is also a common approach.

In terms of physically modelling the systems, there was a divide in the required procedures. To implement a MySQL or Cassandra data model, one must create a structure (tables) to withhold the data. Therefore, to visualise the implementation, one can create diagrams and illustrations of the tables and how they will look and function. Comparatively, the MongoDB and Neo4j systems are created dynamically at the time of load. Meaning that to diagrammatically represent the data model, one must visualise an entry into the database. For example, the MongoDB database contains a collection of documents. In order to identify the structure of the document, I had to physically create an example document. This added to the complexity of the modelling, which as a result increased the amount of time taken to create the prototype systems. Thus, the positives which a flexible and dynamic schema bring, there is also a slight negative which is the increase in modelling difficulty.\\[0.5em]
\end{addmargin}

\textbf{Schema Implementation Complexity}
\begin{addmargin}[2em]{0pt}
A key attribute of NoSQL technologies is their schema-less characteristics. Removing the strict, rigid, structural boundaries which a relational databases imposes allows one to freely adapt and manipulate one's data model with ease. Specifically so within the document-orientated MongoDB environment. Implementing the MongoDB data model, provides one with a sense of control and flexibility, which I found was lacking using a relational database management system.\\[0.5em]
\end{addmargin}

\textbf{Query Language Capabilities}
\begin{addmargin}[2em]{0pt}
Each solution has its own dedicated query language. They all possess functionality which has become standard for any database management system. Neo4j's query language Cypher, is based on SQL constructs, and therefore shares much of the same functional capabilities. Cassandra is most like a relational database in terms of structure; facilitates similar querying abilities to SQL. The MongoDB querying syntax is vastly different, however functionally, it is not dissimilar to the other solutions.

The query testing which was undertaken as part of the project, exposed the limitations of the systems. Basic functional querying was achieved for all of the solutions. I was able to retrieve subsets of data, capable of providing satisfactory analytical insight into the underlying dataset. However, these insights were limited. Using the respective systems querying languages meant I was only able to implement more complex querying such as gene co-expression and calculating transitive closure for Neo4j and MySQL (using a workaround). Admittedly gene co-expression is specific to a biological dataset, however it does have application in other fields.

Neo4j was the only solution which was able to successfully return the expected output for all of the queries. The first four simplistic queries were completed with ease and the more complex, co-expression and transitive closure queries were also achieved. This was as a result of Neo4j's acyclic graph model which allows one to query the path of a relationship.

It is common knowledge that the expressiveness of MySQL systems is limited. Especially so, when defining recursive queries such as transitive closure. Unlike other relational database systems, such as PostgreSQL and DB2 for example, MySQL does not provide the built-in functionality to complete queries such as these. For example, using PostgreSQL, one has the ability to use the ``RECURSIVE'' modifier in a ``WITH'' query. This feature allows one to accomplish results otherwise impossible in standard legacy SQL systems. The ``RECURSIVE'' modifier can use the output of the ``WITH'' query which allows it to query hierarchical data. This is just one alternative relational database system one can use, however there are others which produce similar results. It is often the case with open source software such as the community-edition of MySQL, that there are limitations to what one can achieve. This is a consideration one must decide upon when selecting a database management system. Ensure that the product can facilitate the needs of the requirements or be forced to pay subscription and maintenance fees for a more advanced product. The rights to MySQL were sold to Oracle in 2008, and since then developmental progress seems to have ground to a halt. There has been just one major software release in the past several years. This is a major problem, as there is no official route for developers to discuss the system with Oracle.

The MongoDB and Cassandra systems were unable to successfully complete all of the devised queries. The systems failed to achieve the required output for calculating transitive closure and determining gene co-expression. One reason for this was due to the fact that they lack the querying functionality to bind closely related data. Both have the functional capabilities to join data, but fall short when connecting data recursively. The MongoDB and Cassandra querying languages also cannot calculate relational algebra. This restricts their ability to calculate complex queries such as transitive closure. Thus, returning the hierarchical path of a value for example, was not possible using the query language functionality alone.

It is important to note that, despite the MongoDB and Cassandra systems failing to complete all of the queries, one should not presume achieving the expected result is an impossibility. There are alternative methods for calculating complex queries in these systems. MongoDB and Cassandra both provide API driver documentation. The APIs are available in popular programming languages such as C, C++, Java, Python and Ruby. Providing an API for software engineers to develop their own application is standard for many software companies; including MongoDB and Cassandra. Thus, with suitable pre-processing or sophisticated application code, both systems would be able to successfully achieve all of the queries. However, as data pre-processing and writing application code was out of scope for this project, the MongoDB and Cassandra systems were unable to complete all of the queries.\\[0.5em]
\end{addmargin}

\textbf{Analytical Insight}
\begin{addmargin}[2em]{0pt}
Many of today's largest, most influential and powerful companies in the world are reaping the rewards of collecting and storing big data successfully. This being said, data is useless if it is unclear or impossible to understand. Having the ability to illustrate and visualise data is a key factor in turning data into value. While each of the solutions evaluated all offer a standard flat file export which everyone is familiar with (CSV or equivalent); only Neo4j, the graph-orientated database, delivers a refreshing approach to data visualisation. There is a plethora of third-party applications and programming language APIs which can transform one's data into a visual masterpiece. However, Neo4j uses its property graph model structure, to provide one with an out-of-the-box modelling alternative to previously accepted industry standard traditions.
\end{addmargin}

\textbf{Database Scalability}
\begin{addmargin}[2em]{0pt}
The scalability of the databases was evaluated by loading the EMAPA and EMAGE datasets into each data model. This provided one with an insight into how adequate the solutions are in handling a big dataset. Each database indicated that they are more than capable of scaling to meet the necessary requirements; to store and import a large volume of data.

Each of the systems had the required functionality to load the datasets using a command-line interface. There are multiple methods for loading data each of the databases. However, for this project I was looking to evaluate the basic functional capabilities of the systems. By running two experiments I was able analyse the load times for each of the systems. The load time for the MySQL, MongoDB and Cassandra databases all differed. One reason for this was because of varying dataset sizes. However, the main reason for the disparity in load times was as a result of the NoSQL systems having to impose indexes and constraints at the time of load. The MySQL data model had the tables created and were ready to load data; all that was required was to import the data straight in. Comparatively, the MongoDB and Cassandra systems were creating the structure, imposing constraints and also imposing indexes at the time of load. This was increasing the overall execution of the load.

The most striking difference in load time was the Neo4j data model. When importing the data without pre-defined indexes and constraints, loading the data was taking on average 5.5 hours. After running an experiment to identify the overall effect of imposing indexes on the data model, the load time was cut to just over 2 minutes. The issue seems to have been with the creation of the node relationships. Physically loading the data values into the database took a matter of seconds; joining all of the nodes to one specific value was increasing the load time exponentially.\\[0.5em]
\end{addmargin}

\section{Future Work}\label{futurework}
Although this project has been successful in achieving each of its intended research objectives, it could be further developed in a number of ways:

\begin{itemize}
\item Using a larger dataset with more variety in terms of data type, will further scrutinise the integrity of the database solutions. While the databases were able to cope with the volume of data, the tests did show signs of weakness.
\item Construct more advanced, complex queries to allow one to examine the execution times.
\item Evaluate a variation of the technologies in this project. The database landscape is vast and there are many more database systems which could be used as a basis for further research. For example, NewSQL which seeks to provide one with the same scalability performance of NoSQL solutions with the benefit from ACID transaction guarantees.
\item Assess the usefulness of the data returned with deeper complexity. Each of the solutions evaluated often rely on third-party applications and add ons to make up for their lack of out-of-the-box functionality. Analysing these tools would enhance the justification of using a selected database system.
\end{itemize}

\section{Critique}\label{critique}
The project mainly focused on the capabilities of the data systems in terms of functional ability for a supplied big dataset. Initially it was my intention to scrutinise the data and analyse its availability in more depth. One of the major challenges faced in storing big data is the usefulness of the data itself. The EMAPA and EMAGE datasets are available in a number of file formats, and a closer look into how the extraction and manipulation process is undertaken would have enhanced the understanding of the dataset as a whole.

When discussing the querying performance of a system, one often compares the execution time of queries. The queries included in this project, while returning valuable data, had almost instant execution times. Therefore the disparity in performance of system was minimal. Thus its exclusion from the analysis. As discussed in section \ref{futurework}, devising more complex queries would allow for the comparison of execution time. This would also heighten the scrutiny and examination of the systems.

\section{Thesis Summary}\label{summary}
I developed four prototype data models using both a relational structured database system and leading NoSQL solutions. The data models were examined to identify their positive and negative aspects, using a real-world dataset from the biological field.

Each of the systems were able to meet the required performance specification for storing a large volume of data. While each solution was more than comfortable for storing the EMAPA and EMAGE datasets, the load times for each varied. Neo4j was exposed to show weakness when importing the data without indexes implemented pre-load. MySQL performed the best during the load tests, which aids the argument that it can handle virtually limitless volumes of data. However, in terms of scalability, MySQL seems to struggle when dealing with multiple operations at any one time. MongoDB's ability to take nearly any data and successfully store it with little fuss, gives rise to the fact that its flexibility is one of the key selling points of the system. There was nothing surprising about the results of the load tests for the Cassandra data model. It had to load a larger volume of data and while it did not produce ground-breaking results, it faired averagely.

Much of the project evaluation was focused on the querying functionality each system provides. It seems that database management systems are focused on providing one with the functional requirements to complete basic transactions. Anything more complex is expected to be done using an the APIs available and create an application to model the data in whichever way necessary.

As outlined in section \ref{futurework}, there is scope for taking this project further and in various directions. New technologies are being released constantly, providing one with the ability to collect and store their data in ways which were before impossible. It is important that these technologies remain open source, as progress within paid services can often stagnate and fail to keep up with the pace of development. Big data is an exciting new challenge. However, to be able to profit from this challenge, one needs to ensure they have the infrastructure and management system in place to deal with such complexities. This project has tested the boundaries of some of the most popular database solutions available. It has provided an insight into the analytical capabilities of both industry standard and new technologies. Further research will enhance the functio